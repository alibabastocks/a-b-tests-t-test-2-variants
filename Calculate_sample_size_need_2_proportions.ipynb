{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBS8YVqq38b7KkDbRxnMny",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alibabastocks/a-b-tests-t-test-2-variants/blob/main/Calculate_sample_size_need_2_proportions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample Size Calculator\n",
        "\n",
        "*   For Binary distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "9zJYqrhxyV5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approach explanation:\n",
        "\n",
        "*   Power: This represents the probability of correctly rejecting a false null hypothesis. It's often calculated as one minus beta (Î²), which is the probability of making a Type II error (failing to detect a real difference). A higher power indicates a greater chance of identifying a true effect if it's present in your data.\n",
        "\n",
        "*   Alpha: This represents the probability of rejecting a true null hypothesis. It's often referred to as the significance level. It essentially indicates the risk of getting a false positive result (concluding a difference exists when there truly isn't). Common choices for alpha are 0.05 (5%) or 0.01 (1%).\n",
        "\n",
        "*   Two-tailed test (default approach): It assumes the new variation (B) could be either better or worse than the control (A) on the metric you're testing. It essentially checks for any statistically significant difference between A and B.\n",
        "\n",
        "*   One-tailed test (less common): This is used when you have a strong prior belief that the variation will be better (or worse) than the control. For instance, you might have seen positive results from similar tests in the past.  Here, the test focuses on detecting a difference only in the direction you predicted (e.g., B having a higher conversion rate than A). This approach requires a smaller sample size.\n"
      ],
      "metadata": {
        "id": "VWFbsSzfQXWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.stats.power"
      ],
      "metadata": {
        "id": "7wgNBM3wm9ea"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edit the following fieds with your experiment data.\n",
        "# Enter numbers as decimals, eg 50% becomes 0.5\n",
        "\n",
        "cr=(0.063)  # Baseline, this the the conversion rate in your control group\n",
        "\n",
        "cr_new = (0.0756) # This is the min improvement you are hoping to measure\n",
        "\n",
        "traffic_control = 0.5  # % of traffic you plan to send to the control (A)\n",
        "\n",
        "traffic_variation = 0.5 # % of traffic you plan to send to the new variation (B)\n",
        "\n"
      ],
      "metadata": {
        "id": "BZFBGeY_O0yl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired power (probability of detecting an effect)\n",
        "power = 0.8\n",
        "\n",
        "# Significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Decide whether your test is two or one-sided\n",
        "alternative = \"two-sided\" # Keep as \"two-sided\" if you are good with the default\n",
        "                       # Otherwise set the alternative hypothesis for one-sided test (\"larger\" mean in new variation B, \"smaller\" means in control group A).\n",
        "\n"
      ],
      "metadata": {
        "id": "b_7jHLUNROem"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traffic ratio\n",
        "traffic_ratio = traffic_control/traffic_variation\n",
        "# Effect size (mean difference divided by standard deviation)\n",
        "effect_size_std = sm.stats.proportion_effectsize(cr_new, cr)\n",
        "\n",
        "\n",
        "# Calculate the required sample size per group\n",
        "sample_size = statsmodels.stats.power.TTestIndPower().solve_power(effect_size=effect_size_std, power=power, alpha=alpha, nobs1=None, ratio = traffic_ratio, alternative=alternative)\n",
        "\n",
        "if sample_size is None:\n",
        "  print(\"Sample size calculation failed.\")\n",
        "else:\n",
        "  print(\"Sample size needed per variation is\", round(sample_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjQYmCJ2QIU0",
        "outputId": "4ff76bf6-2309-4ee6-e167-ec643f16b5b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample size needed per variation is 6366\n"
          ]
        }
      ]
    }
  ]
}
